{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn, optim\n",
    "import os \n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import Audio\n",
    "from IPython.utils import io\n",
    "from synthesizer.inference import Synthesizer\n",
    "from encoder import inference as encoder\n",
    "from vocoder import inference as vocoder\n",
    "from pathlib import Path\n",
    "import numpy as np\n",
    "import librosa\n",
    "encoder_weights = Path(\"encoder/saved_models/pretrained.pt\")\n",
    "encoder.load_model(encoder_weights)\n",
    "vocoder_weights = Path(\"vocoder/saved_models/pretrained/pretrained.pt\")\n",
    "syn_dir = Path(\"synthesizer/saved_models/logs-pretrained/taco_pretrained\")\n",
    "encoder.load_model(encoder_weights)\n",
    "synthesizer = Synthesizer(syn_dir)\n",
    "vocoder.load_model(vocoder_weights)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#   in_fpath = Path(\"voicesrc\\\\Pan1.wav\")\n",
    "# subdir='id10286-isKyMAYUOgg-00004'\n",
    "filepath=r'D:\\voiceset\\dataset\\test_wav'\n",
    "in_fpath=os.path.join(filepath,subdir[:7],subdir[8:-6],subdir[-5:]+'.wav')\n",
    "#读取文件\n",
    "file=open('..\\I2E\\ivectortest.txt')\n",
    "file.readlines()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def embedEX(in_fpath):\n",
    "    filepath=r'D:\\voiceset\\dataset\\test_wav'\n",
    "#     filepath=r'D:\\voiceset\\dataset\\wav'\n",
    "    in_fpath=os.path.join(filepath,in_fpath[:7],in_fpath[8:-6],in_fpath[-5:]+'.wav')\n",
    "    reprocessed_wav = encoder.preprocess_wav(in_fpath)\n",
    "    original_wav, sampling_rate = librosa.load(in_fpath)\n",
    "    preprocessed_wav = encoder.preprocess_wav(original_wav, sampling_rate)\n",
    "    embed = encoder.embed_utterance(preprocessed_wav)\n",
    "    return embed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class I2E_Net(torch.nn.Module):\n",
    "    def __init__(self, in_dim, n_hidden_1, out_dim):\n",
    "        super(I2E_Net, self).__init__()\n",
    "        self.layer1 = torch.nn.Sequential(torch.nn.Linear(in_dim, n_hidden_1), torch.nn.ReLU(True))\n",
    "        self.layer2 = torch.nn.Sequential(torch.nn.Linear(n_hidden_1, out_dim))\n",
    "#         self.layer2 = torch.nn.Sequential(torch.nn.Linear(n_hidden_1, n_hidden_2), torch.nn.ReLU(True))\n",
    "#         self.layer3 = torch.nn.Sequential(torch.nn.Linear(n_hidden_2, out_dim))\n",
    " \n",
    "    def forward(self, x):\n",
    "        x = self.layer1(x)\n",
    "        x = self.layer2(x)\n",
    "#         x = self.layer3(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class I2E_Net(torch.nn.Module):\n",
    "    def __init__(self, in_dim, n_hidden_1,n_hidden_2,out_dim):\n",
    "        super(I2E_Net2, self).__init__()\n",
    "        self.layer1 = torch.nn.Sequential(torch.nn.Linear(in_dim, n_hidden_1), torch.nn.ReLU(True))\n",
    "        self.layer2 = torch.nn.Sequential(torch.nn.Linear(n_hidden_1, n_hidden_2), torch.nn.ReLU(True))\n",
    "        self.layer3 = torch.nn.Sequential(torch.nn.Linear(n_hidden_2, out_dim))\n",
    " \n",
    "    def forward(self, x):\n",
    "        x=x/torch.norm(x, p=2)\n",
    "        x = self.layer1(x)\n",
    "        x = self.layer2(x)\n",
    "        x = self.layer3(x)\n",
    "        x=x/torch.norm(x, p=2)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class I2E_Net2MSE(torch.nn.Module):\n",
    "    def __init__(self, in_dim, n_hidden_1,n_hidden_2,out_dim):\n",
    "        super(I2E_Net2, self).__init__()\n",
    "        self.layer1 = torch.nn.Sequential(torch.nn.Linear(in_dim, n_hidden_1), torch.nn.ReLU(True))\n",
    "        self.layer2 = torch.nn.Sequential(torch.nn.Linear(n_hidden_1, n_hidden_2), torch.nn.ReLU(True))\n",
    "        self.layer3 = torch.nn.Sequential(torch.nn.Linear(n_hidden_2, out_dim))\n",
    " \n",
    "    def forward(self, x):\n",
    "        x=x/torch.norm(x, p=2)\n",
    "        x = self.layer1(x)\n",
    "        x = self.layer2(x)\n",
    "        x = self.layer3(x)\n",
    "        x=x/torch.norm(x, p=2)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class I2E_Net3(torch.nn.Module):\n",
    "    def __init__(self):\n",
    "        super(I2E_Net3, self).__init__()\n",
    "        self.layer1 = torch.nn.Sequential(nn.Conv1d(400,350,1), nn.ReLU())\n",
    "        self.layer2 = torch.nn.Sequential(nn.Conv1d(350,300,1), nn.ReLU())\n",
    "#         self.layer3 = torch.nn.Sequential(nn.Conv1d(300,256,1), nn.ReLU())\n",
    "        self.fc = torch.nn.Linear(300,256)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = self.layer1(x)\n",
    "        x = self.layer2(x)\n",
    "        x = x.view(x.size(0), -1)\n",
    "        x = self.fc(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class I2E_Net5(torch.nn.Module):\n",
    "    def __init__(self):\n",
    "        super(I2E_Net5, self).__init__()\n",
    "        self.layer1 = torch.nn.Sequential(nn.Conv2d(1,1,1)), nn.ReLU())\n",
    "        self.layer2 = torch.nn.Sequential(nn.Conv2d(1,1,(1,9))), nn.ReLU())\n",
    "        self.layer2 = torch.nn.Sequential(nn.Conv2d(1,1,(1,9))), nn.ReLU())\n",
    "        self.layer2 = torch.nn.Sequential(nn.Conv2d(1,1,(1,9))), nn.ReLU())\n",
    "        self.layer2 = torch.nn.Sequential(nn.Conv2d(1,1,(1,9))), nn.ReLU())\n",
    "#         self.layer3 = torch.nn.Sequential(nn.Conv1d(300,256,1), nn.ReLU())0\n",
    "        self.fc = torch.nn.Linear(300,256)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = self.layer1(x)\n",
    "        x = self.layer2(x)\n",
    "        x = x.view(x.size(0), -1)\n",
    "        x = self.fc(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class I2E_Net4(torch.nn.Module):\n",
    "    def __init__(self):\n",
    "        super(I2E_Net4, self).__init__()\n",
    "#         ,bidirectional=True\n",
    "        self.lstm = torch.nn.LSTM(400,256,1)\n",
    "        self.linear=torch.nn.Linear(256,256)\n",
    "        self.relu=torch.nn.ReLU()\n",
    "    def forward(self, x):\n",
    "        out,(hidden,cell)= self.lstm(x)\n",
    "#         x= self.relu( self.linear(hidden[-1]))\n",
    "        x= self.relu( self.linear(hidden[-1]))\n",
    "        x=x/torch.norm(x,dim=1,keepdim=True)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 定义一些超参数\n",
    "batch_size = 64\n",
    "learning_rate = 0.02\n",
    "num_epoches = 20"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#模型\n",
    "model=I2E_Net2MSE(400,350,300,256)\n",
    "#损失函数\n",
    "# criterion = torch.nn.MSELoss(reduce=True, size_average=False)\n",
    "criterion = torch.nn.MSELoss(reduction='sum')\n",
    "# criterion = torch.nn.L1Loss(reduction='sum')\n",
    "#优化器\n",
    "# optimizer = optim.SGD(model.parameters(), lr=learning_rate)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 模型\n",
    "# model=I2E_Net3()\n",
    "#损失函数0,\n",
    "model=I2E_Net4()\n",
    "# criterion = torch.nn.MSELoss(reduce=True, size_average=False) reduction='sum'\n",
    "criterion = torch.nn.MSELoss(reduction='sum')\n",
    "#优化器\n",
    "optimizer = optim.SGD(model.parameters(), lr=learning_rate)\n",
    "# optimizer = torch.optim.Adam(model.parameters(), lr=0.001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 训练模型\n",
    "epoch = 0\n",
    "totaldir='D:\\\\pycharm_project\\\\vs2ttsvoice\\\\I2E'\n",
    "#损失值写入文件\n",
    "\n",
    "for i in range(80):\n",
    "    pathstr =str(i+1)+'.txt' \n",
    "    pathstr=os.path.join(totaldir, pathstr)\n",
    "    file=open( pathstr)\n",
    "    for line in file.readlines():\n",
    "        data=line.strip().replace('[','').replace(']','').split()\n",
    "        filename=data[0]\n",
    "        data=data[1:]\n",
    "#         out=[[list(map(float,data))]]\n",
    "#         out=torch.tensor(out)\n",
    "#         out=out.permute(0,2,1)\n",
    "#         pred=model(out)\n",
    "#         pred=pred.squeeze(0)\n",
    "        out=list(map(float,data))\n",
    "        out=torch.tensor(out)\n",
    "        pred=model(out)\n",
    "        label=embedEX(filename)\n",
    "        label=torch.from_numpy(label)\n",
    "        loss = criterion(pred, label)\n",
    "        print_loss = loss.data.item()\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        epoch+=1\n",
    "        if epoch%100 == 0:\n",
    "        #         print('label:{0}\\n,out:{1}\\n,pred:{2}\\n'.format(label.size(),out.size(),pred.size()))\n",
    "        #保存参数、\n",
    "            print('epoch: {}, loss: {:.4}'.format(epoch, loss.data.item()))\n",
    "            state = { 'model':model.state_dict(), 'epoch':epoch }                       \n",
    "            if not os.path.isdir('checkpoint'):\n",
    "                 os.mkdir('checkpoint')    \n",
    "            torch.save(state, '.\\checkpoint\\model4_epoch_%d.ckpt' % (epoch))\n",
    "            file_save = open('loss2.txt',mode='a')\n",
    "            file_save.write('\\n'+'epoch:'+str(epoch)+'  loss:'+str(loss.data.item()))\n",
    "            file_save.close() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 训练模型3,将参数写入文件中\n",
    "epoch = 0\n",
    "totaldir=r\"D:\\pycharm_project\\vs2ttsvoice\\I2E\"\n",
    "#损失值写入文件\n",
    "\n",
    "for i in range(80):\n",
    "    pathstr =str(i+1)\n",
    "    pathstr=os.path.join(totaldir, pathstr)\n",
    "    file=open( pathstr)\n",
    "    for line in file.readlines():\n",
    "        data=line.strip().replace('[','').replace(']','').split()\n",
    "        filename=data[0]\n",
    "        data=data[1:]\n",
    "        out=[[list(map(float,data))]]\n",
    "        out=torch.tensor(out)\n",
    "        out=out.permute(0,2,1)\n",
    "        pred=model(out)\n",
    "        pred=pred.squeeze(0)\n",
    "        label=embedEX(filename)\n",
    "        label=torch.from_numpy(label)\n",
    "        loss = criterion(pred, label)\n",
    "        print_loss = loss.data.item()\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        epoch+=1\n",
    "        if epoch%100 == 0:\n",
    "        #         print('label:{0}\\n,out:{1}\\n,pred:{2}\\n'.format(label.size(),out.size(),pred.size()))\n",
    "        #保存参数、\n",
    "            print('epoch: {}, loss: {:.4}'.format(epoch, loss.data.item()))\n",
    "            state = { 'net':model.state_dict(), 'epoch': epoch, }                                 # 1 、 先建立一个字典\n",
    "            if not os.path.isdir('checkpoint'):\n",
    "                 os.mkdir('checkpoint')       # 2 、 建立一个保存参数的文件夹\n",
    "            torch.save(state, './checkpoint/epoch_%d.ckpt' % (epoch))\n",
    "            #保存损失函数\n",
    "# file_save = open('loss.txt','w')\n",
    "# file_save.write('\\n'+'epoch:'+str(epoch)+'  loss:'+loss.data.item())\n",
    "# file_save.close() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 训练模型4,将参数写入文件中\n",
    "epoch = 0\n",
    "totaldir=r\"D:\\pycharm_project\\vs2ttsvoice\\I2E\"\n",
    "#损失值写入文件\n",
    "\n",
    "for i in range(80):\n",
    "    pathstr =str(i+1)+'.txt'\n",
    "    pathstr=os.path.join(totaldir, pathstr)\n",
    "    file=open( pathstr)\n",
    "    for line in file.readlines():\n",
    "        data=line.strip().replace('[','').replace(']','').split()\n",
    "        filename=data[0]\n",
    "        data=data[1:]\n",
    "        out=[[list(map(float,data))]]\n",
    "        out=torch.tensor(out)\n",
    "        pred=model(out)\n",
    "        pred=pred.squeeze(0)\n",
    "        label=embedEX(filename)\n",
    "        label=torch.from_numpy(label)\n",
    "        loss = criterion(pred, label)\n",
    "        print_loss = loss.data.item()\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        epoch+=1\n",
    "        if epoch%100 == 0:\n",
    "        #         print('label:{0}\\n,out:{1}\\n,pred:{2}\\n'.format(label.size(),out.size(),pred.size()))\n",
    "        #保存参数、\n",
    "            print('epoch: {}, loss: {:.4}'.format(epoch, loss.data.item()))\n",
    "#             state = { 'net':model.state_dict(), 'epoch': epoch, }                                 # 1 、 先建立一个字典\n",
    "#             if not os.path.isdir('checkpoint'):\n",
    "#                  os.mkdir('checkpoint')       # 2 、 建立一个保存参数的文件夹\n",
    "#             torch.save(state, './checkpoint/epoch_%d.ckpt' % (epoch))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 训练模型\n",
    "epoch = 0\n",
    "file=open('..\\I2E\\ivectortest.txt')\n",
    "for line in file.readlines():\n",
    "    data=line.strip().replace('[','').replace(']','').split()\n",
    "    filename=data[0]\n",
    "    data=data[1:]\n",
    "    out=[[list(map(float,data))]]\n",
    "    out=torch.tensor(out)\n",
    "    out=out.permute(0,2,1)\n",
    "    pred=model(out)\n",
    "    pred=pred.squeeze(0)\n",
    "    label=embedEX(filename)\n",
    "    label=torch.from_numpy(label)\n",
    "    loss = criterion(pred, label)\n",
    "    print_loss = loss.data.item()\n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    epoch+=1\n",
    "    print('epoch: {}, loss: {:.4}'.format(epoch, loss.data.item()))\n",
    "#     if epoch%50 == 0:\n",
    "#         print('label:{0}\\n,out:{1}\\n,pred:{2}\\n'.format(label.size(),out.size(),pred.size()))\n",
    "#         print('epoch: {}, loss: {:.4}'.format(epoch, loss.data.item()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "in_fpath=r'D:\\voiceset\\dataset\\test_wav\\id10270\\5r0dWxy17C8\\00003.wav'\n",
    "reprocessed_wav = encoder.preprocess_wav(in_fpath)\n",
    "original_wav, sampling_rate = librosa.load(in_fpath)\n",
    "preprocessed_wav = encoder.preprocess_wav(original_wav, sampling_rate)\n",
    "embed1 = encoder.embed_utterance(preprocessed_wav)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "in_fpath=r'D:\\voiceset\\dataset\\test_wav\\id10270\\5r0dWxy17C8\\00004.wav'\n",
    "reprocessed_wav = encoder.preprocess_wav(in_fpath)\n",
    "original_wav, sampling_rate = librosa.load(in_fpath)\n",
    "preprocessed_wav = encoder.preprocess_wav(original_wav, sampling_rate)\n",
    "embed2 = encoder.embed_utterance(preprocessed_wav)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 读取训练参数\n",
    "# checkpoint = torch.load(r'C:\\NLP\\tts_rtvc\\rtvc\\checkpoint\\model3_epoch_56300.ckpt')\n",
    "checkpoint = torch.load(r'D:\\pycharm_project\\vs2ttsvoice\\checkpoint\\model4_epoch_98800.ckpt')\n",
    "model.load_state_dict(checkpoint['model'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import soundfile as sf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#合成语音\n",
    "from IPython.utils import io\n",
    "epoch=0\n",
    "text=\"This is being said in my own voice.  The computer has learned to do an impression of me.\"\n",
    "file=open('.\\I2E\\ivectortest.txt')\n",
    "filenamelabel=''\n",
    "for line in file.readlines():\n",
    "# line=file.readline()\n",
    "    data=line.strip().replace('[','').replace(']','').split()\n",
    "    filename=data[0]\n",
    "    if (filenamelabel==filename[:7]):\n",
    "        continue\n",
    "    print(filename)\n",
    "    data=data[1:]\n",
    "#     out=[[list(map(float,data))]]\n",
    "#     out=torch.tensor(out)\n",
    "#     out=out.permute(0,2,1)\n",
    "#     pred=model(out)\n",
    "#     pred=pred.squeeze(0)\n",
    "#     pred=pred.tolist()\n",
    "#     print(\"pred\",pred)\n",
    "    out=list(map(float,data))\n",
    "    out=torch.tensor(out)\n",
    "    pred=model(out)\n",
    "    pred=pred.tolist()\n",
    "    print(\"pred\",pred)\n",
    "    print(\"Synthesizing 预测语音...\")\n",
    "    with io.capture_output() as captured:\n",
    "      specs = synthesizer.synthesize_spectrograms([text], [pred])\n",
    "    generated_wav = vocoder.infer_waveform(specs[0])\n",
    "    generated_wav = np.pad(generated_wav, (0, synthesizer.sample_rate), mode=\"constant\")\n",
    "    display(Audio(generated_wav, rate=synthesizer.sample_rate))\n",
    "    sf.write(filename[:7]+\"A.wav\", generated_wav, synthesizer.sample_rate)\n",
    "    epoch =+1\n",
    "    #原始声音合成\n",
    "    print('&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&')\n",
    "    label=embedEX(filename)\n",
    "    print(\"Synthesizing 原始语音...\")\n",
    "    print('阈值：',np.linalg.norm(pred-label))\n",
    "    with io.capture_output() as captured:\n",
    "       specs = synthesizer.synthesize_spectrograms([text], [label])\n",
    "    generated_wav = vocoder.infer_waveform(specs[0])\n",
    "    generated_wav = np.pad(generated_wav, (0, synthesizer.sample_rate), mode=\"constant\")\n",
    "    display(Audio(generated_wav, rate=synthesizer.sample_rate))\n",
    "    filenamelabel=filename[:7]\n",
    "    sf.write(filename[:7]+\".wav\", generated_wav, synthesizer.sample_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#合成测试集所有的语音\n",
    "from IPython.utils import io\n",
    "epoch=0\n",
    "text=\"This is being said in my own voice.The computer has learned to do an impression of me.\"\n",
    "file=open('.\\I2E\\ivectortest.txt')\n",
    "filesynVS=r'D:\\voiceset\\dataset\\testVS_wav'\n",
    "filesynI2E=r'D:\\voiceset\\dataset\\testI2E_wav'\n",
    "filenamelabel=''\n",
    "for line in file.readlines():\n",
    "# line=file.readline()\n",
    "    data=line.strip().replace('[','').replace(']','').split()\n",
    "    filename=data[0]\n",
    "    #建立VS目录in_fpath=os.path.join(filepath,in_fpath[:7],in_fpath[8:-6],in_fpath[-5:]+'.wav')\n",
    "    pathvs=os.path.join(filesynVS,filename[:7],filename[8:-6])\n",
    "    if not os.path.exists(pathvs):\n",
    "        os.makedirs(pathvs)\n",
    "        \n",
    "    #创建i2目录    \n",
    "    pathi2e=os.path.join(filesynI2E,filename[:7],filename[8:-6])        \n",
    "    if not os.path.exists(pathi2e):\n",
    "        os.makedirs(pathi2e)\n",
    "\n",
    "    data=data[1:]\n",
    "    out=list(map(float,data))\n",
    "    out=torch.tensor(out)\n",
    "    pred=model(out)\n",
    "    pred=pred.tolist()\n",
    "#     print(\"pred\",pred)\n",
    "    print(\"Synthesizing 预测语音...\")\n",
    "    with io.capture_output() as captured:\n",
    "      specs = synthesizer.synthesize_spectrograms([text], [pred])\n",
    "    generated_wav = vocoder.infer_waveform(specs[0])\n",
    "    generated_wav = np.pad(generated_wav, (0, synthesizer.sample_rate), mode=\"constant\")\n",
    "    display(Audio(generated_wav, rate=synthesizer.sample_rate))\n",
    "    \n",
    "    sf.write(os.path.join(pathi2e,filename[-5:]+'.wav'), generated_wav, synthesizer.sample_rate)\n",
    "    epoch =+1\n",
    "    #原始声音合成\n",
    "    print('&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&')\n",
    "    label=embedEX(filename)\n",
    "    print(\"Synthesizing 原始语音...\")\n",
    "    print('阈值：',np.linalg.norm(pred-label))\n",
    "    with io.capture_output() as captured:\n",
    "       specs = synthesizer.synthesize_spectrograms([text], [label])\n",
    "    generated_wav = vocoder.infer_waveform(specs[0])\n",
    "    generated_wav = np.pad(generated_wav, (0, synthesizer.sample_rate), mode=\"constant\")\n",
    "    display(Audio(generated_wav, rate=synthesizer.sample_rate))\n",
    "    filenamelabel=filename[:7]\n",
    "    sf.write(os.path.join(pathvs,filename[-5:]+'.wav'), generated_wav, synthesizer.sample_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
